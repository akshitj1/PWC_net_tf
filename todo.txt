------- TODO
keep corr_vol in same order in both if else conditions
loss function to pyramidal loss
disparity maxes till 24?
lazy execution for speedup
downsample gt to 1/4 res and upsample to get upperbound on loss
-------- DONE
optimizers rms prop, adam
check how flow is compounded
unit test warp function
reduce max disp to 4
predict half res and bilinear interpolate
add regularization in loss? - Not done in tf impl.
adjust learning rate, epsilon for adam to 1e-4
assert lossfn shapes match: https://github.com/keras-team/keras/issues/5441
replace resize with crop and pad - resize_with_crop_pad works as expected
adapt samples, input size to kitti 2012
optimize to batch size 8
single train example: 
- 1e-4 lr converges much faster to 1.2 AEPE, 80% than 1e-3 which is much much slower (1.8k vs 5k)
- single very long epoch reduces loss monotonically with lot fewer oscillations than 1 epoch per step but is much slower to converge
he normal initializer makes training unstable? replace with default
